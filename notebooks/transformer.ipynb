{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization, Dropout, MultiHeadAttention\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('/mnt/data/data_with_state.csv')\n",
    "\n",
    "# Convert the 'time' column to datetime and sort the data by time\n",
    "data['time'] = pd.to_datetime(data['time'], format='%d-%m-%Y')\n",
    "data = data.sort_values('time')\n",
    "\n",
    "# Select features and target\n",
    "features = ['lat', 'lon', 'rtzsm_inst', 'sfsm_inst', 'week_no']\n",
    "target = 'gws_inst'\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data[features + [target]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to create sequences for Transformer\n",
    "def create_sequences(data, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        xs.append(data[i:i+seq_length, :-1])  # All features except target\n",
    "        ys.append(data[i+seq_length, -1])     # Target column (gws_inst)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "SEQ_LENGTH = 60  # Experiment with sequence length\n",
    "X, y = create_sequences(data_scaled, SEQ_LENGTH)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Transformer model parameters\n",
    "d_model = 64  # Dimension of embedding\n",
    "num_heads = 4  # Number of attention heads\n",
    "ff_dim = 128  # Hidden layer size in feed-forward network\n",
    "dropout_rate = 0.2\n",
    "\n",
    "# Build the Transformer model\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Multi-Head Self Attention\n",
    "    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(inputs, inputs)\n",
    "    attention_output = Dropout(dropout)(attention_output)\n",
    "    attention_output = LayerNormalization(epsilon=1e-6)(attention_output + inputs)\n",
    "\n",
    "    # Feed Forward layer\n",
    "    ff_output = Dense(ff_dim, activation=\"relu\")(attention_output)\n",
    "    ff_output = Dropout(dropout)(ff_output)\n",
    "    ff_output = Dense(inputs.shape[-1])(ff_output)\n",
    "   \n",
    "    return LayerNormalization(epsilon=1e-6)(ff_output + attention_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Input shape: (SEQ_LENGTH, number of features)\n",
    "inputs = Input(shape=(SEQ_LENGTH, X_train.shape[2]))\n",
    "\n",
    "# Transformer encoder block\n",
    "x = transformer_encoder(inputs, head_size=d_model, num_heads=num_heads, ff_dim=ff_dim, dropout=dropout_rate)\n",
    "\n",
    "# Global Average Pooling to collapse sequence dimension\n",
    "x = Dense(20, activation=\"relu\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# Output layer for regression\n",
    "outputs = Dense(1)(x)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss='mean_squared_error')\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plot training & validation loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Transformer Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Rescale the predictions back to the original scale\n",
    "y_test_rescaled = scaler.inverse_transform(np.concatenate([X_test[:, -1], y_test.reshape(-1,1)], axis=1))[:, -1]\n",
    "predictions_rescaled = scaler.inverse_transform(np.concatenate([X_test[:, -1], predictions], axis=1))[:, -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_test_rescaled, label='Actual Groundwater Storage')\n",
    "plt.plot(predictions_rescaled, label='Predicted Groundwater Storage', alpha=0.7)\n",
    "plt.title('Transformer: Actual vs Predicted Groundwater Storage')\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Groundwater Storage')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
